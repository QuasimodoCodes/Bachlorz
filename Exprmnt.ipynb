{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r\"C:\\Users\\spide\\OneDrive\\Desktop\\Bachlorz\\extracted_data.csv\")\n",
    "\n",
    "# Load pre-trained multilingual BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Load Norwegian stopwords from NLTK\n",
    "norwegian_stopwords = set(stopwords.words('norwegian'))\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in norwegian_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to generate embeddings for a given text\n",
    "def generate_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Apply stopword removal to the content\n",
    "df['content'] = df['content'].apply(remove_stopwords)\n",
    "\n",
    "# Generate embeddings for each tender content\n",
    "df['embeddings'] = df['content'].apply(lambda x: generate_embeddings(x))\n",
    "\n",
    "# Function to handle search query and find the most relevant tenders\n",
    "def recommend_tenders(query, df):\n",
    "    query = remove_stopwords(query)  # Remove stopwords from the query as well\n",
    "    query_embedding = generate_embeddings(query)\n",
    "    \n",
    "    # Compute similarity scores between the query and each document\n",
    "    similarities = df['embeddings'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    \n",
    "    # Sort tenders based on similarity and return the top 5 recommendations\n",
    "    top_recommendations = df.iloc[similarities.argsort()[-5:]]\n",
    "    return top_recommendations[['filename', 'source_format', 'content']]\n",
    "\n",
    "# Example usage\n",
    "query = \"cloud computing\"\n",
    "recommended_tenders = recommend_tenders(query, df)\n",
    "print(recommended_tenders)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import gradio as gr\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"C:/Users/spide/OneDrive/Desktop/Bachlorz/extracted_data.csv\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate embeddings for a given text\n",
    "def generate_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Generate embeddings for each tender content\n",
    "df['embeddings'] = df['content'].apply(lambda x: generate_embeddings(x))\n",
    "\n",
    "# Function to handle search query and find the most relevant tenders\n",
    "def recommend_tenders(query, df):\n",
    "    query_embedding = generate_embeddings(query)\n",
    "    \n",
    "    # Compute similarity scores between the query and each document\n",
    "    similarities = df['embeddings'].apply(lambda x: cosine_similarity([query_embedding], [x])[0][0])\n",
    "    \n",
    "    # Sort tenders based on similarity and return the top 5 recommendations\n",
    "    top_recommendations = df.iloc[similarities.argsort()[-5:]]\n",
    "    \n",
    "    # Add cosine similarity score to the result\n",
    "    top_recommendations['cosine_similarity'] = similarities[similarities.argsort()[-5:]]\n",
    "    \n",
    "    return top_recommendations[['filename', 'source_format', 'content', 'cosine_similarity']]\n",
    "\n",
    "# Gradio function for displaying recommendations\n",
    "def recommend_tenders_gradio(query):\n",
    "    recommended_tenders = recommend_tenders(query, df)\n",
    "    return recommended_tenders\n",
    "\n",
    "# Create Gradio Interface\n",
    "interface = gr.Interface(fn=recommend_tenders_gradio, inputs=\"text\", outputs=\"dataframe\", title=\"Tender Recommendation System\", description=\"Enter a search query to find the most relevant tenders.\")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Gradio function for displaying recommendations\n",
    "def recommend_tenders_gradio(query):\n",
    "    recommended_tenders = recommend_tenders(query, df)\n",
    "    return recommended_tenders\n",
    "\n",
    "# Create Gradio Interface\n",
    "interface = gr.Interface(fn=recommend_tenders_gradio, inputs=\"text\", outputs=\"dataframe\", live=True, title=\"Tender Recommendation System\", description=\"Enter a search query to find the most relevant tenders.\")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" query = \"cloud computing krav\" \n",
    "query_embedding = generate_embeddings(query)\n",
    "\n",
    "# Compare similarity\n",
    "similarity = cosine_similarity([query_embedding])\n",
    "print(\"Cosine Similarity:\", similarity[0][0])  # Expect a value between 0 and 1 \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946de19bee3b4423a73067ecd406875d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spide\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\spide\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e9239b2834410ca1a28701d9a4f4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9803c8ee341a4c39aa2a6837f8342d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a640528f52724328bce4d22f0d8a9c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c93c63ccbe4c99ac460b971004792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Nedre Romerike brann- og redningsvesen IKS Rev...   \n",
      "1  Unnamed: 0 Unnamed: 1 Unnamed: 2 Unnamed: 3 Un...   \n",
      "2  Vedlegg II.01 Bilag 2 - Seriøsitetskrav for by...   \n",
      "3  MAL – Retningslinjer og krav til prøvedrift PR...   \n",
      "4  KONKURRANSEGRUNNLAG ÅPEN ANBUDSKONKURRANSE ett...   \n",
      "\n",
      "                                     cleaned_content  \\\n",
      "0  Nedre Romerike brann og redningsvesen IKS Revi...   \n",
      "1  Unnamed 0 Unnamed 1 Unnamed 2 Unnamed 3 Unname...   \n",
      "2  Vedlegg II01 Bilag 2  Seriøsitetskrav for bygg...   \n",
      "3  MAL  Retningslinjer og krav til prøvedrift PRO...   \n",
      "4  KONKURRANSEGRUNNLAG ÅPEN ANBUDSKONKURRANSE ett...   \n",
      "\n",
      "                                     bert_embeddings  \n",
      "0  [[tensor(-0.0473), tensor(-0.2503), tensor(0.4...  \n",
      "1  [[tensor(0.2136), tensor(-0.0183), tensor(0.13...  \n",
      "2  [[tensor(-0.2064), tensor(-0.4413), tensor(0.0...  \n",
      "3  [[tensor(0.0175), tensor(-0.2266), tensor(0.55...  \n",
      "4  [[tensor(0.1483), tensor(-0.2654), tensor(0.54...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the CSV file into the DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\spide\\OneDrive\\Desktop\\Bachlorz\\Data\\extracted_data.csv\")\n",
    "\n",
    "# Step 2: Text Cleanup Function (removes unwanted characters, excessive spaces)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function cleans the extracted text by removing unwanted characters,\n",
    "    excessive white spaces, and special characters.\n",
    "    \"\"\"\n",
    "    # Remove excessive white spaces (replace multiple spaces with one)\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    \n",
    "    # Remove leading/trailing spaces\n",
    "    text = text.strip()  \n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 3: BERT Tokenizer and Model Setup (using a multilingual version of BERT)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    \"\"\"\n",
    "    This function takes cleaned text as input, tokenizes it using BERT's tokenizer,\n",
    "    and returns the contextual embeddings for the text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling of token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Step 4: Clean the text in the 'content' column\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Step 5: Generate BERT embeddings for the cleaned content\n",
    "df['bert_embeddings'] = df['cleaned_content'].apply(get_bert_embeddings)\n",
    "\n",
    "# Check the results\n",
    "print(df[['content', 'cleaned_content', 'bert_embeddings']].head())\n",
    "\n",
    "# Optional: Save the DataFrame with embeddings back to a new CSV\n",
    "df.to_csv(r\"C:\\Users\\spide\\OneDrive\\Desktop\\Bachlorz\\extracted_data_with_embeddings.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
